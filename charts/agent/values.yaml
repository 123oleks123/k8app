# Default values for agent.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
info:
  name: agent
  license: PLZ_CHANGE
  namespace: PLZ_CHANGE
  deployment_environment: PLZ_CHANGE

IsDaemonSet: false

gateway:
  addr: 127.0.0.1:4317

replicaCount: 1

service:
  type: ClusterIP
  ports:
    - port: 4317
      name: otelgrpc
    - port: 4318
      name: otelhttp
#    - port: 24224
#      name: fluentforward

rbac:
  create: true

podSecurityPolicy:
  enabled: false

image:
  repository: otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.72.0"

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "collector"
  # Annotations to add to the service account
  annotations: { }

podAnnotations: { }

podSecurityContext: { }
# fsGroup: 2000

securityContext: { }
# capabilities:
#   drop:
#   - ALL
# readOnlyRootFilesystem: true
# runAsNonRoot: true
# runAsUser: 1000

resources:
  limits:
    cpu: 1
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 0.5Gi
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: { }

tolerations: [ ]

affinity: { }


# required AWS SSM parameter store certificates
certs:
  "ca.crt": "ca.crt"
  "client.crt": "client.crt"
  "client.key": "client.key"

receivers:
  prometheus:
    default:
      enabled: true
      scrape_configs:
          # dashboard: ID: 17728 https://grafana.com/grafana/dashboards/17728-opentelemetry-collector-0-68-0-v20221229/
#          - job_name: "{{ .Values.info.name }}"
#            scrape_interval: 15s
#            static_configs:
#              - targets: [ '0.0.0.0:8888' ]
    append:
      enabled: false
      scrape_configs: []

    kubernetes:
      enabled: true
      scrape_configs:
        - job_name: "{{ .Values.info.name }}"
          scrape_interval: 15s
          honor_labels: false
          kubernetes_sd_configs:
            - role: pod
              selectors:
                - role: pod
                  # only scrape data from pods running on the same node as collector
                  field: "spec.nodeName=$NODE_NAME"
          scheme: http
          relabel_configs:
            - action: keep
              source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_instance
                - __meta_kubernetes_pod_labelpresent_app_kubernetes_io_instance
              regex: (agent);true
            - action: keep
              source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - __meta_kubernetes_pod_labelpresent_app_kubernetes_io_name
              regex: (agent);true
            - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
            - action: drop
              source_labels:
                - __meta_kubernetes_pod_phase
              regex: (Failed|Succeeded)
        # required helm name: node-exporter
        # example: up{container="node-exporter",instance="10.0.2.193:9100",job="node-exporter-prometheus-node-exporter-svg2b",namespace="monitoring",pod="node-exporter-prometheus-node-exporter-svg2b"} 1 1672147386763
        #       => up{deployment_environment="test",instance="10.0.2.193:9100",job="node-exporter-prometheus-node-exporter-svg2b",k8s_container_name="node-exporter",k8s_daemonset_name="node-exporter-prometheus-node-exporter",k8s_namespace_name="monitoring",k8s_node_name="ip-10-0-2-193.eu-central-1.compute.internal",k8s_pod_name="node-exporter-prometheus-node-exporter-svg2b",k8s_pod_uid="c2c6ae55-a746-43cf-bc4c-7b1af050726a",license="test",namespace="test",net_host_name="10.0.2.193",origin_prometheus="prometheus01",service_instance_id="10.0.2.193:9100",service_name="node-exporter-prometheus-node-exporter-svg2b"} 1 1672154051968
        #       => up{deployment_environment="test",instance="node-exporter-prometheus-node-exporter-svg2b",job="node-exporter-prometheus-node-exporter-svg2b",k8s_container="node-exporter",k8s_namespace="monitoring",license="test",namespace="test",net_host_name="10.0.2.193",nodename="ip-10-0-2-193.eu-central-1.compute.internal",origin_prometheus="test_test_test",service_instance_id="10.0.2.193:9100",service_name="serviceMonitor/monitoring/prometheus-prometheus-node-exporter"} 1 1672156467807
        #
        # dashboard:ID: 11074 https://grafana.com/grafana/dashboards/11074-node-exporter-for-prometheus-dashboard-en-v20201010/
        # dashboard: ID: 1860 https://grafana.com/grafana/dashboards/1860-node-exporter-full/
        - job_name: prometheus-node-exporter
          #scrape_interval: 15s
          honor_labels: false
          kubernetes_sd_configs:
            - role: pod
              selectors:
                - role: pod
                  # only scrape data from pods running on the same node as collector
                  field: "spec.nodeName=$NODE_NAME"
          scheme: http
          relabel_configs:
            - action: keep
              source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_instance
                - __meta_kubernetes_pod_labelpresent_app_kubernetes_io_instance
              regex: (node-exporter);true
            - action: keep
              source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - __meta_kubernetes_pod_labelpresent_app_kubernetes_io_name
              regex: (prometheus-node-exporter);true
            - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
            - action: drop
              source_labels:
                - __meta_kubernetes_pod_phase
              regex: (Failed|Succeeded)
            - action: replace
              replacement: "{{ .Values.info.namespace }}/{{ .Values.info.license }}/{{ .Values.info.deployment_environment }}"
              target_label: origin_prometheus
          metric_relabel_configs: [ ]
        # required helm name: kube-state-metrics
        # https://grafana.com/grafana/dashboards/13332-kube-state-metrics-v2/
        # dashboard ID: 13332
        - job_name: kube-state-metrics
          honor_labels: false
          kubernetes_sd_configs:
            - role: pod
              selectors:
                - role: pod
                  # only scrape data from pods running on the same node as collector
                  field: "spec.nodeName=$NODE_NAME"
          relabel_configs:
            - action: keep
              source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_instance
                - __meta_kubernetes_pod_labelpresent_app_kubernetes_io_instance
              regex: (kube-state-metrics);true
            - action: keep
              source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - __meta_kubernetes_pod_labelpresent_app_kubernetes_io_name
              regex: (kube-state-metrics);true
            - action: drop
              source_labels:
                - __meta_kubernetes_pod_phase
              regex: (Failed|Succeeded)
            - action: replace
              replacement: "{{ .Values.info.namespace }}/{{ .Values.info.license }}/{{ .Values.info.deployment_environment }}"
              target_label: cluster
        # coredns
        # https://grafana.com/grafana/dashboards/14981-coredns/
        # dashboard ID: 14981
        # https://grafana.com/grafana/dashboards/15762-kubernetes-system-coredns/
        # dashboard ID: 15762
        - job_name: coredns
          honor_labels: false
          kubernetes_sd_configs:
            - role: pod
              selectors:
                - role: pod
                  # only scrape data from pods running on the same node as collector
                  field: "spec.nodeName=$NODE_NAME"
          relabel_configs:
            - action: keep
              source_labels: # k8s-app: kube-dns
                - __meta_kubernetes_pod_label_k8s_app
                - __meta_kubernetes_pod_labelpresent_k8s_app
              regex: (kube-dns);true
            - source_labels: [ __address__ ]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?
              replacement: $$1:9153
            - action: drop
              source_labels:
                - __meta_kubernetes_pod_phase
              regex: (Failed|Succeeded)
        - job_name: 'kubernetes-kubelet'
          scrape_interval: 1m
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
            - role: node
          relabel_configs:
            - target_label: __address__
              replacement: kubernetes.default.svc.cluster.local:443
            - source_labels: [ __meta_kubernetes_node_name ]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$$1/proxy/metrics
        - job_name: 'kubernetes-resource'
          scrape_interval: 15s
          scheme: https
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          kubernetes_sd_configs:
            - role: node
          relabel_configs:
            - target_label: __address__
              replacement: kubernetes.default.svc.cluster.local:443
            - source_labels: [ __meta_kubernetes_node_name ]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$$1/proxy/metrics/resource
            - action: replace
              replacement: "{{ .Values.info.namespace }}/{{ .Values.info.license }}/{{ .Values.info.deployment_environment }}"
              target_label: cluster
        # search pods containing prometheus.io/scrape annotation
        #
        # example:
        # prometheus.io/path: /metrics
        # prometheus.io/port: "7777"
        # prometheus.io/scrape: "true"
        # prometheus.io/param: "xxx=yyy"
        - job_name: kubernetes-pods
          kubernetes_sd_configs:
            - role: pod
              selectors:
                - role: pod
                  # only scrape data from pods running on the same node as collector
                  field: "spec.nodeName=$NODE_NAME"
          honor_labels: false
          relabel_configs:
            - action: keep
              regex: true
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scrape
            - action: replace
              regex: (https?)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels:
                - __address__
                - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            #            - action: labelmap
            #              regex: __meta_kubernetes_pod_label_(.+)
            - action: drop
              regex: Pending|Succeeded|Failed|Completed
              source_labels:
                - __meta_kubernetes_pod_phase
config:
  receivers:
    otlp:
      protocols:
        http: {}
        grpc:
          max_recv_msg_size_mib: 32
          max_concurrent_streams: 16
    fluentforward:
      endpoint: 0.0.0.0:24224
    filelog:
      include:
        - /var/log/pods/*/*/*.log
      exclude:
        # Exclude logs from all containers named otel-collector
        - /var/log/pods/*/otel-collector/*.log
      start_at: end
      include_file_path: true
      include_file_name: false
      operators:
        # Find out which format is used by kubernetes
        - type: router
          id: get-format
          routes:
            - output: parser-docker
              expr: 'body matches "^\\{"'
            - output: parser-crio
              expr: 'body matches "^[^ Z]+ "'
            - output: parser-containerd
              expr: 'body matches "^[^ Z]+Z"'
        # Parse CRI-O format
        - type: regex_parser
          id: parser-crio
          regex: '^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout_type: gotime
            layout: '2006-01-02T15:04:05.000000000-07:00'
        # Parse CRI-Containerd format
        - type: regex_parser
          id: parser-containerd
          regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        # Parse Docker format
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        - type: move
          from: attributes.log
          to: body
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
        # Rename attributes
        - type: move
          from: attributes.stream
          to: attributes["log.iostream"]
        - type: move
          from: attributes.container_name
          to: resource["k8s.container.name"]
        - type: move
          from: attributes.namespace
          to: resource["k8s.namespace.name"]
        - type: move
          from: attributes.pod_name
          to: resource["k8s.pod.name"]
        - type: move
          from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
        - type: move
          from: attributes.uid
          to: resource["k8s.pod.uid"]
  exporters:
    prometheus:
      endpoint: "0.0.0.0:9000"
      send_timestamps: true
      metric_expiration: 60m
      resource_to_telemetry_conversion:
        enabled: true
    logging:
      verbosity: 0
      sampling_initial: 10
      sampling_thereafter: 50
    otlp/global:
      endpoint: "{{ .Values.gateway.addr }}"
      compression: snappy
      tls:
        ca_file: /certs/ca.crt
        cert_file: /certs/client.crt
        key_file: /certs/client.key
        server_name_override:  xxx
      sending_queue:
        enabled: true
        num_consumers: 20
        queue_size: 1000
      retry_on_failure:
        enabled: false
        initial_interval: 1s
        max_interval: 10s
        max_elapsed_time: 20s
  service:
    extensions: [ pprof, zpages, health_check ]
    pipelines:
      traces:
        receivers: [ otlp ]
        processors: [ resource, resource/tempo, memory_limiter, batch ]
        exporters: [ logging, otlp/global ]
      metrics/2:
        receivers: [ prometheus ]
        processors: [ resource, transform, memory_limiter, batch ]
        exporters: [ logging, otlp/global ]
      metrics:
        receivers: [ otlp,  ]
        processors: [ resource, transform, memory_limiter, batch ]
        exporters: [ logging, otlp/global ]
      logs:
        receivers: [ fluentforward, otlp, filelog ]
        processors: [ k8sattributes, resource, attributes/loki, resource/loki, transform, memory_limiter, batch ]
        exporters: [ logging, otlp/global ]
  extensions:
    health_check: {}
    pprof:
      endpoint: :1888
    zpages:
      endpoint: :55679
  processors:
    # k8sattributes processor to get the metadata from K8s
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      extract:
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.cluster.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
      # Pod association using resource attributes and connection
      pod_association:
        - sources:
            - from: resource_attribute
              name: ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: host.name
        - sources:
            - from: connection
              name: ip
    # loki not support keys with dots - so replace it
    transform:
      metric_statements:
        - context: resource
          statements:
            - replace_all_patterns(attributes, "key", "k8s.(statefulset|deployment|daemonset).name", "resource")
            - replace_all_patterns(attributes, "key", "service.name", "job")
            - delete_key(attributes, "k8s.pod.uid")
            - delete_key(attributes, "k8s.container.restart_count")
            - replace_all_patterns(attributes, "key", "k8s.node.name", "nodename")
            - replace_all_patterns(attributes, "key", "k8s.namespace.name", "k8s_namespace")
            - replace_all_patterns(attributes, "key", "k8s.pod.name", "instance")
            - replace_all_patterns(attributes, "key", "k8s.container.name", "k8s_container")
      log_statements:
        - context: resource
          statements:
            - delete_key(attributes, "k8s.pod.uid")
            - delete_key(attributes, "k8s.container.restart_count")
            - replace_all_patterns(attributes, "key", "k8s.namespace.name", "k8s_namespace")
            - replace_all_patterns(attributes, "key", "k8s.pod.name", "instance")
            - replace_all_patterns(attributes, "key", "k8s.container.name", "k8s_container")
#            - replace_all_patterns(attributes, "key", "\\.", "_")
        - context: log
          statements:
            - set(resource.attributes["loki.format"], "logfmt") where severity_text != "" # backport compatibility tel < 2.2.0
            - delete_key(attributes, "logtag")
            - delete_key(attributes, "log.file.path")
#            - replace_all_patterns(attributes, "key", "\\.", "_")
    resource/tempo:
      attributes:
        - key: "service.name"
          from_attribute: "service"
          action: insert
        - key: "service"
          action: delete
    attributes/loki:
      actions:
        - action: insert
          key: loki.attribute.labels
          value: level
    resource/loki:
      attributes:
#        - action: insert
#          key: loki.format
#          value: logfmt
        - action: insert
          key: loki.resource.labels
          value: resource, service, namespace, license, deployment_environment, k8s_namespace, k8s_container, aws_region
    resource:
      attributes:
        - action: upsert
          key: host.name
          from_attribute: "k8s.node.name"
        - key: namespace
          value: "{{ .Values.info.namespace }}"
          action: upsert
        - key: license
          action: upsert
          value: "{{ .Values.info.license }}"
        - key: deployment_environment
          value: "{{ .Values.info.deployment_environment }}"
          action: upsert
        - key: http.scheme
          action: delete
        - key: net.host.port
          action: delete
        - key: host
          from_attribute: "host.name"
          action: upsert
    batch:
      send_batch_size: 100
      send_batch_max_size: 1000
      timeout: 15ms
    # Enabling the memory_limiter is strongly recommended for every pipeline.
    # Configuration is based on the amount of memory allocated to the collector.
    # The configuration below assumes 2GB of memory. In general, the ballast
    # should be set to 1/3 of the collector's memory, the limit should be 90% of
    # the collector's memory up to 2GB, and the spike should be 25% of the
    # collector's memory up to 2GB. In addition, the "--mem-ballast-size-mib" CLI
    # flag must be set to the same value as the "ballast_size_mib". For more
    # information, see
    # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md
    memory_limiter:
      check_interval: 1s
      limit_mib: 1000
      spike_limit_mib: 300